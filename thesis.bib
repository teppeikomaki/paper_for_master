
@article{hua2021learning,
  title     = {Learning for a robot: Deep reinforcement learning, imitation learning, transfer learning},
  author    = {Hua, Jiang and Zeng, Liangcai and Li, Gongfa and Ju, Zhaojie},
  journal   = {Sensors},
  volume    = {21},
  number    = {4},
  pages     = {1278},
  year      = {2021},
  publisher = {MDPI}
}

@inproceedings{pertsch2021accelerating,
  title        = {Accelerating reinforcement learning with learned skill priors},
  author       = {Pertsch, Karl and Lee, Youngwoon and Lim, Joseph},
  booktitle    = {Conference on robot learning},
  pages        = {188--204},
  year         = {2021},
  organization = {PMLR}
}

@article{singh2020parrot,
  title   = {Parrot: Data-driven behavioral priors for reinforcement learning},
  author  = {Singh, Avi and Liu, Huihan and Zhou, Gaoyue and Yu, Albert and Rhinehart, Nicholas and Levine, Sergey},
  journal = {arXiv preprint arXiv:2011.10024},
  year    = {2020}
}

@article{dinh2016density,
  title   = {Density estimation using real nvp},
  author  = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  journal = {arXiv preprint arXiv:1605.08803},
  year    = {2016}
}

@inproceedings{kumar2014real,
  title        = {Real-time behaviour synthesis for dynamic hand-manipulation},
  author       = {Kumar, Vikash and Tassa, Yuval and Erez, Tom and Todorov, Emanuel},
  booktitle    = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
  pages        = {6808--6815},
  year         = {2014},
  organization = {IEEE}
}


@article{shaw2023leap,
  title   = {Leap hand: Low-cost, efficient, and anthropomorphic hand for robot learning},
  author  = {Shaw, Kenneth and Agarwal, Ananye and Pathak, Deepak},
  journal = {arXiv preprint arXiv:2309.06440},
  year    = {2023}
}

@inproceedings{ahn2020robel,
  title        = {Robel: Robotics benchmarks for learning with low-cost robots},
  author       = {Ahn, Michael and Zhu, Henry and Hartikainen, Kristian and Ponte, Hugo and Gupta, Abhishek and Levine, Sergey and Kumar, Vikash},
  booktitle    = {Conference on robot learning},
  pages        = {1300--1313},
  year         = {2020},
  organization = {PMLR}
}

@article{SAC2,
  title        = {Soft Actor-Critic Algorithms and Applications},
  url          = {http://arxiv.org/abs/1812.05905},
  abstract     = {Model-free deep reinforcement learning ({RL}) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic ({SAC}), our recently introduced off-policy actor-critic algorithm based on the maximum entropy {RL} framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend {SAC} to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate {SAC} on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, {SAC} achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that {SAC} is a promising candidate for learning in real-world robotics tasks.},
  journaltitle = {{arXiv}:1812.05905 [cs, stat]},
  author       = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  urldate      = {2022-01-27},
  date         = {2019-01-29},
  eprinttype   = {arxiv},
  eprint       = {1812.05905},
  file         = {Haarnoja et al_2019_Soft Actor-Critic Algorithms and Applications.pdf:/Users/admin/Dropbox/zotero/Haarnoja et al_2019_Soft Actor-Critic Algorithms and Applications.pdf:application/pdf}
}



@inproceedings{SAC1,
  title      = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  url        = {https://proceedings.mlr.press/v80/haarnoja18b.html},
  shorttitle = {Soft Actor-Critic},
  abstract   = {Model-free deep reinforcement learning ({RL}) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep {RL} algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep {RL} methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  eventtitle = {International Conference on Machine Learning},
  pages      = {1861--1870},
  booktitle  = {Proceedings of the 35th International Conference on Machine Learning},
  publisher  = {{PMLR}},
  author     = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  urldate    = {2022-01-27},
  date       = {2018-07-03},
  langid     = {english},
  note       = {{ISSN}: 2640-3498},
  file       = {Haarnoja et al_2018_Soft Actor-Critic.pdf:/Users/admin/Dropbox/zotero/Haarnoja et al_2018_Soft Actor-Critic3.pdf:application/pdf;Supplementary PDF:/Users/admin/Zotero/storage/DMCQN5AE/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf:application/pdf}
}

@article{MaxEntIRL,
  title    = {Maximum Entropy Inverse Reinforcement Learning},
  abstract = {Recent research has shown the beneﬁt of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-deﬁned, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods.},
  pages    = {6},
  author   = {Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},
  langid   = {english},
  file     = {Ziebart et al. - Maximum Entropy Inverse Reinforcement Learning.pdf:/Users/admin/Zotero/storage/UZJKDLRH/Ziebart et al. - Maximum Entropy Inverse Reinforcement Learning.pdf:application/pdf}
}

@inproceedings{Mujoco,
  title      = {{MuJoCo}: A physics engine for model-based control},
  doi        = {10.1109/IROS.2012.6386109},
  shorttitle = {{MuJoCo}},
  abstract   = {We describe a new physics engine tailored to model-based control. Multi-joint dynamics are represented in generalized coordinates and computed via recursive algorithms. Contact responses are computed via efficient new algorithms we have developed, based on the modern velocity-stepping approach which avoids the difficulties with spring-dampers. Models are specified using either a high-level C++ {API} or an intuitive {XML} file format. A built-in compiler transforms the user model into an optimized data structure used for runtime computation. The engine can compute both forward and inverse dynamics. The latter are well-defined even in the presence of contacts and equality constraints. The model can include tendon wrapping as well as actuator activation states (e.g. pneumatic cylinders or muscles). To facilitate optimal control applications and in particular sampling and finite differencing, the dynamics can be evaluated for different states and controls in parallel. Around 400,000 dynamics evaluations per second are possible on a 12-core machine, for a 3D homanoid with 18 dofs and 6 active contacts. We have already used the engine in a number of control applications. It will soon be made publicly available.},
  eventtitle = {2012 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},
  pages      = {5026--5033},
  booktitle  = {2012 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},
  author     = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  date       = {2012-10},
  note       = {{ISSN}: 2153-0866}
}

@article{DQN,
  title    = {Playing Atari with Deep Reinforcement Learning},
  abstract = {We present the ﬁrst deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We ﬁnd that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  pages    = {9},
  author   = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  langid   = {english},
  file     = {Mnih et al. - Playing Atari with Deep Reinforcement Learning.pdf:/Users/admin/Zotero/storage/PKR87HXP/Mnih et al. - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf}
}

@inproceedings{softQ,
  title      = {Reinforcement Learning with Deep Energy-Based Policies},
  url        = {https://proceedings.mlr.press/v70/haarnoja17a.html},
  abstract   = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
  eventtitle = {International Conference on Machine Learning},
  pages      = {1352--1361},
  booktitle  = {Proceedings of the 34th International Conference on Machine Learning},
  publisher  = {{PMLR}},
  author     = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  urldate    = {2022-01-27},
  date       = {2017-07-17},
  langid     = {english},
  note       = {{ISSN}: 2640-3498},
  file       = {Haarnoja et al_2017_Reinforcement Learning with Deep Energy-Based Policies.pdf:/Users/admin/Dropbox/zotero/Haarnoja et al_2017_Reinforcement Learning with Deep Energy-Based Policies2.pdf:application/pdf;Supplementary PDF:/Users/admin/Zotero/storage/9V9JEFJ6/Haarnoja et al. - 2017 - Reinforcement Learning with Deep Energy-Based Poli.pdf:application/pdf}
}

@book{RLBook,
  edition    = {2nd ed},
  title      = {Reinforcement learning : an introduction},
  url        = {https://ci.nii.ac.jp/ncid/BB27095177},
  shorttitle = {Reinforcement learning},
  abstract   = {Reinforcement learning : an introduction Richard S. Sutton and Andrew G. Barto （Adaptive computation and machine learning） {MIT} Press, c2018 2nd ed},
  publisher  = {{MIT} Press},
  author     = {Sutton, Richard S. and Barto, Andrew G.},
  urldate    = {2022-01-27},
  date       = {2018},
  langid     = {japanese}
}



@inproceedings{doubleQ,
  title      = {Addressing Function Approximation Error in Actor-Critic Methods},
  url        = {https://proceedings.mlr.press/v80/fujimoto18a.html},
  abstract   = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of {OpenAI} gym tasks, outperforming the state of the art in every environment tested.},
  eventtitle = {International Conference on Machine Learning},
  pages      = {1587--1596},
  booktitle  = {Proceedings of the 35th International Conference on Machine Learning},
  publisher  = {{PMLR}},
  author     = {Fujimoto, Scott and Hoof, Herke and Meger, David},
  urldate    = {2022-01-28},
  date       = {2018-07-03},
  langid     = {english},
  note       = {{ISSN}: 2640-3498},
  file       = {Fujimoto et al_2018_Addressing Function Approximation Error in Actor-Critic Methods.pdf:/Users/admin/Dropbox/zotero/Fujimoto et al_2018_Addressing Function Approximation Error in Actor-Critic Methods.pdf:application/pdf;Supplementary PDF:/Users/admin/Zotero/storage/WAW4NZEG/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf:application/pdf}
}

@book{Okaya2022DeepLearning,
  title     = {深層学習 改訂第2版(機械学習プロフェッショナルシリーズ)},
  author    = {岡谷貴之},
  year      = {2022},
  publisher = {講談社}
}