
@article{DexMV,
	title = {{DexMV}: Imitation Learning for Dexterous Manipulation from Human Videos},
	url = {http://arxiv.org/abs/2108.05877},
	shorttitle = {{DexMV}},
	abstract = {While significant progress has been made on understanding hand-object interactions in computer vision, it is still very challenging for robots to perform complex dexterous manipulation. In this paper, we propose a new platform and pipeline {DexMV} (Dexterous Manipulation from Videos) for imitation learning. We design a platform with: (i) a simulation system for complex dexterous manipulation tasks with a multi-finger robot hand and (ii) a computer vision system to record large-scale demonstrations of a human hand conducting the same tasks. In our novel pipeline, we extract 3D hand and object poses from videos, and propose a novel demonstration translation method to convert human motion to robot demonstrations. We then apply and compare multiple imitation learning algorithms with the demonstrations. We show that the demonstrations can indeed improve robot learning by a large margin and solve the complex tasks which reinforcement learning alone cannot solve. Project page with video: https://yzqin.github.io/dexmv},
	journaltitle = {{arXiv}:2108.05877 [cs]},
	author = {Qin, Yuzhe and Wu, Yueh-Hua and Liu, Shaowei and Jiang, Hanwen and Yang, Ruihan and Fu, Yang and Wang, Xiaolong},
	urldate = {2022-01-24},
	date = {2021-12-02},
	eprinttype = {arxiv},
	eprint = {2108.05877},
	file = {Qin et al_2021_DexMV.pdf:/Users/admin/Dropbox/zotero/Qin et al_2021_DexMV.pdf:application/pdf},
}

@inproceedings{ADROIT,
	location = {Karlsruhe, Germany},
	title = {Fast, strong and compliant pneumatic actuation for dexterous tendon-driven hands},
	isbn = {978-1-4673-5643-5 978-1-4673-5641-1},
	url = {http://ieeexplore.ieee.org/document/6630771/},
	doi = {10.1109/ICRA.2013.6630771},
	abstract = {We describe a pneumatic actuation system for dexterous robotic hands. It was motivated by our desire to improve the {ShadowHand} system, yet it is quite universal and indeed we are already using it with a second robotic hand we have developed. Our actuation system allows us to move the {ShadowHand} skeleton faster than a human hand (70 msec limit-to-limit movement, 30 msec overall reﬂex latency), generate sufﬁcient forces (40 N at each ﬁnger tendon, 125N at each wrist tendon), and achieve high compliance on the mechanism level (6 grams of external force at the ﬁngertip displaces the ﬁnger when the system is powered.) This combination of speed, force and compliance is a prerequisite for dexterous manipulation, yet it has never before been achieved with a tendon-driven system, let alone a system with 24 degrees of freedom and 40 tendons.},
	eventtitle = {2013 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {1512--1519},
	booktitle = {2013 {IEEE} International Conference on Robotics and Automation},
	publisher = {{IEEE}},
	author = {Kumar, Vikash and Xu, Zhe and Todorov, Emanuel},
	urldate = {2022-01-25},
	date = {2013-05},
	langid = {english},
	file = {Kumar et al. - 2013 - Fast, strong and compliant pneumatic actuation for.pdf:/Users/admin/Zotero/storage/N2T5KGE9/Kumar et al. - 2013 - Fast, strong and compliant pneumatic actuation for.pdf:application/pdf},
}

@article{surveyRobot2,
	title = {Trends and challenges in robot manipulation},
	volume = {364},
	doi = {10.1126/science.aat8414},
	abstract = {Dexterous manipulation is one of the primary goals in robotics. Robots with this capability could sort and package objects, chop vegetables, and fold clothes. As robots come to work side by side with humans, they must also become human-aware. Over the past decade, research has made strides toward these goals. Progress has come from advances in visual and haptic perception and in mechanics in the form of soft actuators that offer a natural compliance. Most notably, immense progress in machine learning has been leveraged to encapsulate models of uncertainty and to support improvements in adaptive and robust control. Open questions remain in terms of how to enable robots to deal with the most unpredictable agent of all, the human.},
	pages = {eaat8414},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Billard, Aude and Kragic, Danica},
	date = {2019-06},
}

@inproceedings{DRLforRobot1,
	title = {Learning robot in-hand manipulation with tactile features},
	doi = {10.1109/HUMANOIDS.2015.7363524},
	abstract = {Dexterous manipulation enables repositioning of objects and tools within a robot's hand. When applying dexterous manipulation to unknown objects, exact object models are not available. Instead of relying on models, compliance and tactile feedback can be exploited to adapt to unknown objects. However, compliant hands and tactile sensors add complexity and are themselves difficult to model. Hence, we propose acquiring in-hand manipulation skills through reinforcement learning, which does not require analytic dynamics or kinematics models. In this paper, we show that this approach successfully acquires a tactile manipulation skill using a passively compliant hand. Additionally, we show that the learned tactile skill generalizes to novel objects.},
	eventtitle = {2015 {IEEE}-{RAS} 15th International Conference on Humanoid Robots (Humanoids)},
	pages = {121--127},
	booktitle = {2015 {IEEE}-{RAS} 15th International Conference on Humanoid Robots (Humanoids)},
	author = {van Hoof, Herke and Hermans, Tucker and Neumann, Gerhard and Peters, Jan},
	date = {2015-11},
	file = {van Hoof et al_2015_Learning robot in-hand manipulation with tactile features.pdf:/Users/admin/Dropbox/zotero/van Hoof et al_2015_Learning robot in-hand manipulation with tactile features.pdf:application/pdf},
}

@inproceedings{DRLforRobot2,
	title = {Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},
	doi = {10.1109/ICRA.2017.7989385},
	abstract = {Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations.},
	eventtitle = {2017 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {3389--3396},
	booktitle = {2017 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	author = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
	date = {2017-05},
	file = {Gu et al_2017_Deep reinforcement learning for robotic manipulation with asynchronous.pdf:/Users/admin/Dropbox/zotero/Gu et al_2017_Deep reinforcement learning for robotic manipulation with asynchronous.pdf:application/pdf},
}

@inproceedings{NPG,
	title = {A Natural Policy Gradient},
	volume = {14},
	url = {https://proceedings.neurips.cc/paper/2001/hash/4b86abe48d358ecf194c56c69108433e-Abstract.html},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {{MIT} Press},
	author = {Kakade, Sham M},
	urldate = {2022-01-26},
	date = {2002},
	file = {Kakade_2002_A Natural Policy Gradient.pdf:/Users/admin/Dropbox/zotero/Kakade_2002_A Natural Policy Gradient.pdf:application/pdf},
}

@article{SurveyRobot1,
	title = {Learning for a Robot: Deep Reinforcement Learning, Imitation Learning, Transfer Learning},
	volume = {21},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/21/4/1278},
	doi = {10.3390/s21041278},
	shorttitle = {Learning for a Robot},
	abstract = {Dexterous manipulation of the robot is an important part of realizing intelligence, but manipulators can only perform simple tasks such as sorting and packing in a structured environment. In view of the existing problem, this paper presents a state-of-the-art survey on an intelligent robot with the capability of autonomous deciding and learning. The paper first reviews the main achievements and research of the robot, which were mainly based on the breakthrough of automatic control and hardware in mechanics. With the evolution of artificial intelligence, many pieces of research have made further progresses in adaptive and robust control. The survey reveals that the latest research in deep learning and reinforcement learning has paved the way for highly complex tasks to be performed by robots. Furthermore, deep reinforcement learning, imitation learning, and transfer learning in robot control are discussed in detail. Finally, major achievements based on these methods are summarized and analyzed thoroughly, and future research challenges are proposed.},
	pages = {1278},
	number = {4},
	journaltitle = {Sensors},
	author = {Hua, Jiang and Zeng, Liangcai and Li, Gongfa and Ju, Zhaojie},
	urldate = {2022-01-26},
	date = {2021-01},
	langid = {english},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	file = {Hua et al_2021_Learning for a Robot.pdf:/Users/admin/Dropbox/zotero/Hua et al_2021_Learning for a Robot.pdf:application/pdf},
}

@article{surveyIL,
	title = {Survey of imitation learning for robotic manipulation},
	volume = {3},
	issn = {2366-5971, 2366-598X},
	url = {http://link.springer.com/10.1007/s41315-019-00103-5},
	doi = {10.1007/s41315-019-00103-5},
	abstract = {With the development of robotics, the application of robots has gradually evolved from industrial scenes to more intelligent service scenarios. For multitasking operations of robots in complex and uncertain environments, the traditional manual coding method is not only cumbersome but also unable to adapt to sudden changes in the environment. Imitation learning that avoids learning skills from scratch by using the expert demonstration has become the most effective way for robotic manipulation. The paper is intended to provide the survey of imitation learning of robotic manipulation and explore the future research trend. The review of the art of imitation learning for robotic manipulation involves three aspects that are demonstration, representation and learning algorithms. Towards the end of the paper, we highlight areas of future research potential.},
	pages = {362--369},
	number = {4},
	journaltitle = {International Journal of Intelligent Robotics and Applications},
	shortjournal = {Int J Intell Robot Appl},
	author = {Fang, Bin and Jia, Shidong and Guo, Di and Xu, Muhua and Wen, Shuhuan and Sun, Fuchun},
	urldate = {2022-01-27},
	date = {2019-12},
	langid = {english},
	file = {Fang et al. - 2019 - Survey of imitation learning for robotic manipulat.pdf:/Users/admin/Zotero/storage/V7JWILWK/Fang et al. - 2019 - Survey of imitation learning for robotic manipulat.pdf:application/pdf},
}

@article{ROBEL,
	title = {{ROBEL}: Robotics Benchmarks for Learning with Low-Cost Robots},
	url = {http://arxiv.org/abs/1909.11639},
	shorttitle = {{ROBEL}},
	abstract = {{ROBEL} is an open-source platform of cost-effective robots designed for reinforcement learning in the real world. {ROBEL} introduces two robots, each aimed to accelerate reinforcement learning research in different task domains: D'Claw is a three-fingered hand robot that facilitates learning dexterous manipulation tasks, and D'Kitty is a four-legged robot that facilitates learning agile legged locomotion tasks. These low-cost, modular robots are easy to maintain and are robust enough to sustain on-hardware reinforcement learning from scratch with over 14000 training hours registered on them to date. To leverage this platform, we propose an extensible set of continuous control benchmark tasks for each robot. These tasks feature dense and sparse task objectives, and additionally introduce score metrics as hardware-safety. We provide benchmark scores on an initial set of tasks using a variety of learning-based methods. Furthermore, we show that these results can be replicated across copies of the robots located in different institutions. Code, documentation, design files, detailed assembly instructions, final policies, baseline details, task videos, and all supplementary materials required to reproduce the results are available at www.roboticsbenchmarks.org.},
	journaltitle = {{arXiv}:1909.11639 [cs, stat]},
	author = {Ahn, Michael and Zhu, Henry and Hartikainen, Kristian and Ponte, Hugo and Gupta, Abhishek and Levine, Sergey and Kumar, Vikash},
	urldate = {2022-01-27},
	date = {2019-12-15},
	eprinttype = {arxiv},
	eprint = {1909.11639},
	file = {Ahn et al_2019_ROBEL.pdf:/Users/admin/Dropbox/zotero/Ahn et al_2019_ROBEL3.pdf:application/pdf},
}

@article{SAC2,
	title = {Soft Actor-Critic Algorithms and Applications},
	url = {http://arxiv.org/abs/1812.05905},
	abstract = {Model-free deep reinforcement learning ({RL}) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic ({SAC}), our recently introduced off-policy actor-critic algorithm based on the maximum entropy {RL} framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend {SAC} to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate {SAC} on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, {SAC} achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that {SAC} is a promising candidate for learning in real-world robotics tasks.},
	journaltitle = {{arXiv}:1812.05905 [cs, stat]},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
	urldate = {2022-01-27},
	date = {2019-01-29},
	eprinttype = {arxiv},
	eprint = {1812.05905},
	file = {Haarnoja et al_2019_Soft Actor-Critic Algorithms and Applications.pdf:/Users/admin/Dropbox/zotero/Haarnoja et al_2019_Soft Actor-Critic Algorithms and Applications.pdf:application/pdf},
}

@inproceedings{GAIL,
	title = {Generative Adversarial Imitation Learning},
	volume = {29},
	url = {https://papers.nips.cc/paper/2016/hash/cc7e2b878868cbae992d1fb743995d8f-Abstract.html},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Ermon, Stefano},
	urldate = {2022-01-27},
	date = {2016},
	file = {Ho_Ermon_2016_Generative Adversarial Imitation Learning.pdf:/Users/admin/Dropbox/zotero/Ho_Ermon_2016_Generative Adversarial Imitation Learning.pdf:application/pdf},
}

@inproceedings{SOIL,
	title = {State-Only Imitation Learning for Dexterous Manipulation},
	doi = {10.1109/IROS51168.2021.9636557},
	abstract = {Modern model-free reinforcement learning methods have recently demonstrated impressive results on a number of problems. However, complex domains like dexterous manipulation remain a challenge due to the high sample complexity. To address this, current approaches employ expert demonstrations in the form of state-action pairs, which are difficult to obtain for real-world settings such as learning from videos. In this paper, we move toward a more realistic setting and explore state-only imitation learning. To tackle this setting, we train an inverse dynamics model and use it to predict actions for state-only demonstrations. The inverse dynamics model and the policy are trained jointly. Our method performs on par with state-action approaches and considerably outperforms {RL} alone. By not relying on expert actions, we are able to learn from demonstrations with different dynamics, morphologies, and objects. Videos available on the \${\textbackslash}textproject page\$.},
	eventtitle = {2021 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	pages = {7865--7871},
	booktitle = {2021 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems ({IROS})},
	author = {Radosavovic, Ilija and Wang, Xiaolong and Pinto, Lerrel and Malik, Jitendra},
	date = {2021-09},
	note = {{ISSN}: 2153-0866},
	file = {Radosavovic et al_2021_State-Only Imitation Learning for Dexterous Manipulation.pdf:/Users/admin/Dropbox/zotero/Radosavovic et al_2021_State-Only Imitation Learning for Dexterous Manipulation2.pdf:application/pdf},
}

@inproceedings{SAC1,
	title = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
	url = {https://proceedings.mlr.press/v80/haarnoja18b.html},
	shorttitle = {Soft Actor-Critic},
	abstract = {Model-free deep reinforcement learning ({RL}) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep {RL} algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep {RL} methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
	eventtitle = {International Conference on Machine Learning},
	pages = {1861--1870},
	booktitle = {Proceedings of the 35th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	urldate = {2022-01-27},
	date = {2018-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Haarnoja et al_2018_Soft Actor-Critic.pdf:/Users/admin/Dropbox/zotero/Haarnoja et al_2018_Soft Actor-Critic3.pdf:application/pdf;Supplementary PDF:/Users/admin/Zotero/storage/DMCQN5AE/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf:application/pdf},
}

@article{MaxEntIRL,
	title = {Maximum Entropy Inverse Reinforcement Learning},
	abstract = {Recent research has shown the beneﬁt of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-deﬁned, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods.},
	pages = {6},
	author = {Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},
	langid = {english},
	file = {Ziebart et al. - Maximum Entropy Inverse Reinforcement Learning.pdf:/Users/admin/Zotero/storage/UZJKDLRH/Ziebart et al. - Maximum Entropy Inverse Reinforcement Learning.pdf:application/pdf},
}

@inproceedings{Mujoco,
	title = {{MuJoCo}: A physics engine for model-based control},
	doi = {10.1109/IROS.2012.6386109},
	shorttitle = {{MuJoCo}},
	abstract = {We describe a new physics engine tailored to model-based control. Multi-joint dynamics are represented in generalized coordinates and computed via recursive algorithms. Contact responses are computed via efficient new algorithms we have developed, based on the modern velocity-stepping approach which avoids the difficulties with spring-dampers. Models are specified using either a high-level C++ {API} or an intuitive {XML} file format. A built-in compiler transforms the user model into an optimized data structure used for runtime computation. The engine can compute both forward and inverse dynamics. The latter are well-defined even in the presence of contacts and equality constraints. The model can include tendon wrapping as well as actuator activation states (e.g. pneumatic cylinders or muscles). To facilitate optimal control applications and in particular sampling and finite differencing, the dynamics can be evaluated for different states and controls in parallel. Around 400,000 dynamics evaluations per second are possible on a 12-core machine, for a 3D homanoid with 18 dofs and 6 active contacts. We have already used the engine in a number of control applications. It will soon be made publicly available.},
	eventtitle = {2012 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},
	pages = {5026--5033},
	booktitle = {2012 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},
	author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
	date = {2012-10},
	note = {{ISSN}: 2153-0866},
}

@article{SQIL,
	title = {{SQIL}: {IMITATION} {LEARNING} {VIA} {REINFORCEMENT} {LEARNING} {WITH} {SPARSE} {REWARDS}},
	abstract = {Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning ({BC}) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning ({RL}), such as inverse {RL} and generative adversarial imitation learning ({GAIL}), overcome this issue by training an {RL} agent to match the demonstrations over a long horizon. Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often using complex and brittle approximation techniques that involve adversarial training. We propose a simple alternative that still uses {RL}, but does not require learning a reward function. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. We accomplish this by giving the agent a constant reward of r = +1 for matching the demonstrated action in a demonstrated state, and a constant reward of r = 0 for all other behavior. Our method, which we call soft Q imitation learning ({SQIL}), can be implemented with a handful of minor modiﬁcations to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, we show that {SQIL} can be interpreted as a regularized variant of {BC} that uses a sparsity prior to encourage long-horizon imitation. Empirically, we show that {SQIL} outperforms {BC} and achieves competitive results compared to {GAIL}, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and {MuJoCo}. This paper is a proof of concept that illustrates how a simple imitation method based on {RL} with constant rewards can be as effective as more complex methods that use learned rewards.},
	pages = {14},
	author = {Reddy, Siddharth and Dragan, Anca D and Levine, Sergey},
	date = {2020},
	langid = {english},
	file = {Reddy et al. - 2020 - SQIL IMITATION LEARNING VIA REINFORCEMENT LEARNIN.pdf:/Users/admin/Zotero/storage/MIZE8B6C/Reddy et al. - 2020 - SQIL IMITATION LEARNING VIA REINFORCEMENT LEARNIN.pdf:application/pdf},
}

@inproceedings{ALVINN,
	title = {{ALVINN}: An Autonomous Land Vehicle in a Neural Network},
	volume = {1},
	url = {https://proceedings.neurips.cc/paper/1988/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html},
	shorttitle = {{ALVINN}},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Morgan-Kaufmann},
	author = {Pomerleau, Dean A.},
	urldate = {2022-01-27},
	date = {1989},
	file = {Pomerleau_1989_ALVINN.pdf:/Users/admin/Dropbox/zotero/Pomerleau_1989_ALVINN.pdf:application/pdf},
}

@inproceedings{BC1,
	title = {A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning},
	url = {https://proceedings.mlr.press/v15/ross11a.html},
	abstract = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem. [pdf]},
	eventtitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
	pages = {627--635},
	booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
	publisher = {{JMLR} Workshop and Conference Proceedings},
	author = {Ross, Stephane and Gordon, Geoffrey and Bagnell, Drew},
	urldate = {2022-01-27},
	date = {2011-06-14},
	langid = {english},
	note = {{ISSN}: 1938-7228},
	file = {Ross et al_2011_A Reduction of Imitation Learning and Structured Prediction to No-Regret Online.pdf:/Users/admin/Dropbox/zotero/Ross et al_2011_A Reduction of Imitation Learning and Structured Prediction to No-Regret Online.pdf:application/pdf},
}

@inproceedings{BC2,
	title = {Efficient Reductions for Imitation Learning},
	url = {https://proceedings.mlr.press/v9/ross10a.html},
	abstract = {Imitation Learning, while applied successfully on many large real-world problems, is typically addressed as a standard supervised learning problem, where it is assumed the training and testing data are i.i.d..  This is not true in imitation learning as the learned policy influences the future test inputs (states) upon which it will be tested. We show that this leads to compounding errors and a regret bound that grows quadratically in the time horizon of the task. We propose two alternative algorithms for imitation learning where training occurs over several episodes of interaction. These two approaches share in common that the learner’s policy is slowly modified from executing the expert’s policy to the learned policy. We show that this leads to stronger performance guarantees and demonstrate the improved performance on two challenging problems: training a learner to play 1) a 3D racing game (Super Tux Kart) and 2) Mario Bros.; given input images from the games and corresponding actions taken by a human expert and near-optimal planner respectively.},
	eventtitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	pages = {661--668},
	booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	publisher = {{JMLR} Workshop and Conference Proceedings},
	author = {Ross, Stephane and Bagnell, Drew},
	urldate = {2022-01-27},
	date = {2010-03-31},
	langid = {english},
	note = {{ISSN}: 1938-7228},
	file = {Ross_Bagnell_2010_Efficient Reductions for Imitation Learning.pdf:/Users/admin/Dropbox/zotero/Ross_Bagnell_2010_Efficient Reductions for Imitation Learning.pdf:application/pdf},
}

@article{GAN,
	title = {Generative adversarial networks},
	volume = {63},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3422622},
	doi = {10.1145/3422622},
	abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks ({GANs}) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but {GANs} are among the most successful generative models (especially in terms of their ability to generate realistic highresolution images). {GANs} have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
	pages = {139--144},
	number = {11},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2022-01-27},
	date = {2020-10-22},
	langid = {english},
	file = {Goodfellow et al. - 2020 - Generative adversarial networks.pdf:/Users/admin/Zotero/storage/UFMUBR6K/Goodfellow et al. - 2020 - Generative adversarial networks.pdf:application/pdf},
}

@article{LfD,
	title = {Recent Advances in Robot Learning from Demonstration},
	volume = {3},
	url = {https://doi.org/10.1146/annurev-control-100819-063206},
	doi = {10.1146/annurev-control-100819-063206},
	abstract = {In the context of robotics and automation, learning from demonstration ({LfD}) is the paradigm in which robots acquire new skills by learning to imitate an expert. The choice of {LfD} over other robot learning methods is compelling when ideal behavior can be neither easily scripted (as is done in traditional robot programming) nor easily defined as an optimization problem, but can be demonstrated. While there have been multiple surveys of this field in the past, there is a need for a new one given the considerable growth in the number of publications in recent years. This review aims to provide an overview of the collection of machine-learning methods used to enable a robot to learn from and imitate a teacher. We focus on recent advancements in the field and present an updated taxonomy and characterization of existing methods. We also discuss mature and emerging application areas for {LfD} and highlight the significant challenges that remain to be overcome both in theory and in practice.},
	pages = {297--330},
	number = {1},
	journaltitle = {Annual Review of Control, Robotics, and Autonomous Systems},
	author = {Ravichandar, Harish and Polydoros, Athanasios S. and Chernova, Sonia and Billard, Aude},
	urldate = {2022-01-27},
	date = {2020},
	note = {\_eprint: https://doi.org/10.1146/annurev-control-100819-063206},
	file = {Ravichandar et al_2020_Recent Advances in Robot Learning from Demonstration.pdf:/Users/admin/Dropbox/zotero/Ravichandar et al_2020_Recent Advances in Robot Learning from Demonstration.pdf:application/pdf},
}

@article{DQN,
	title = {Playing Atari with Deep Reinforcement Learning},
	abstract = {We present the ﬁrst deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We ﬁnd that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	pages = {9},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	langid = {english},
	file = {Mnih et al. - Playing Atari with Deep Reinforcement Learning.pdf:/Users/admin/Zotero/storage/PKR87HXP/Mnih et al. - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf},
}

@inproceedings{softQ,
	title = {Reinforcement Learning with Deep Energy-Based Policies},
	url = {https://proceedings.mlr.press/v70/haarnoja17a.html},
	abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
	eventtitle = {International Conference on Machine Learning},
	pages = {1352--1361},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
	urldate = {2022-01-27},
	date = {2017-07-17},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Haarnoja et al_2017_Reinforcement Learning with Deep Energy-Based Policies.pdf:/Users/admin/Dropbox/zotero/Haarnoja et al_2017_Reinforcement Learning with Deep Energy-Based Policies2.pdf:application/pdf;Supplementary PDF:/Users/admin/Zotero/storage/9V9JEFJ6/Haarnoja et al. - 2017 - Reinforcement Learning with Deep Energy-Based Poli.pdf:application/pdf},
}

@book{RLBook,
	edition = {2nd ed},
	title = {Reinforcement learning : an introduction},
	url = {https://ci.nii.ac.jp/ncid/BB27095177},
	shorttitle = {Reinforcement learning},
	abstract = {Reinforcement learning : an introduction Richard S. Sutton and Andrew G. Barto （Adaptive computation and machine learning） {MIT} Press, c2018 2nd ed},
	publisher = {{MIT} Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	urldate = {2022-01-27},
	date = {2018},
	langid = {japanese},
}

@inproceedings{DAPG,
	title = {Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations},
	isbn = {978-0-9923747-4-7},
	url = {http://www.roboticsproceedings.org/rss14/p49.pdf},
	doi = {10.15607/RSS.2018.XIV.049},
	abstract = {Dexterous multi-ﬁngered hands are extremely versatile and provide a generic way to perform a multitude of tasks in human-centric environments. However, effectively controlling them remains challenging due to their high dimensionality and large number of potential contacts. Deep reinforcement learning ({DRL}) provides a model-agnostic approach to control complex dynamical systems, but has not been shown to scale to highdimensional dexterous manipulation. Furthermore, deployment of {DRL} on physical systems remains challenging due to sample inefﬁciency. Consequently, the success of {DRL} in robotics has thus far been limited to simpler manipulators and tasks. In this work, we show that model-free {DRL} can effectively scale up to complex manipulation tasks with a high-dimensional 24-{DoF} hand, and solve them from scratch in simulated experiments. Furthermore, with the use of a small number of human demonstrations, the sample complexity can be signiﬁcantly reduced, which enables learning with sample sizes equivalent to a few hours of robot experience. The use of demonstrations result in policies that exhibit very natural movements and, surprisingly, are also substantially more robust. We demonstrate successful policies for object relocation, in-hand manipulation, tool use, and door opening, which are shown in the supplementary video.},
	eventtitle = {Robotics: Science and Systems 2018},
	booktitle = {Robotics: Science and Systems {XIV}},
	publisher = {Robotics: Science and Systems Foundation},
	author = {Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey},
	urldate = {2022-01-27},
	date = {2018-06-26},
	langid = {english},
	file = {Rajeswaran et al. - 2018 - Learning Complex Dexterous Manipulation with Deep .pdf:/Users/admin/Zotero/storage/BQ3TKDBF/Rajeswaran et al. - 2018 - Learning Complex Dexterous Manipulation with Deep .pdf:application/pdf},
}

@inproceedings{IRL,
  title={Algorithms for inverse reinforcement learning.},
  author={Ng, Andrew Y and Russell, Stuart J and others},
  booktitle={Icml},
  volume={1},
  pages={2},
  year={2000}
}

@inproceedings{doubleQ,
	title = {Addressing Function Approximation Error in Actor-Critic Methods},
	url = {https://proceedings.mlr.press/v80/fujimoto18a.html},
	abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of {OpenAI} gym tasks, outperforming the state of the art in every environment tested.},
	eventtitle = {International Conference on Machine Learning},
	pages = {1587--1596},
	booktitle = {Proceedings of the 35th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Fujimoto, Scott and Hoof, Herke and Meger, David},
	urldate = {2022-01-28},
	date = {2018-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Fujimoto et al_2018_Addressing Function Approximation Error in Actor-Critic Methods.pdf:/Users/admin/Dropbox/zotero/Fujimoto et al_2018_Addressing Function Approximation Error in Actor-Critic Methods.pdf:application/pdf;Supplementary PDF:/Users/admin/Zotero/storage/WAW4NZEG/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf:application/pdf},
}